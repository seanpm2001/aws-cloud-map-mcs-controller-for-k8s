{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Kubernetes, with it's implementation of the cluster construct has simplified the ability to schedule workloads across a collection of VMs or nodes. Declarative configuration, immutability, auto-scaling, and self healing have vastly simplified the paradigm of workload management within the cluster - which has enabled teams to move at increasing velocities.</p> <p>As the rate of Kubernetes adoption continues to increase, there has been a corresponding increase in the number of use cases that require workloads to break through the perimeter of the single cluster construct. Requirements concerning workload location/proximity, isolation, and reliability have been the primary catalyst for the emergence of deployment scenarios where a single logical workload will span multiple Kubernetes clusters:</p> <ul> <li>Location based concerns include network latency requirements (e.g. bringing the application as close to users as possible), data gravity requirements (e.g. bringing elements of the application as close to fixed data sources as possible), and jurisdiction based requirements (e.g. data residency limitations imposed via governing bodies);</li> <li>Isolation based concerns include performance (e.g. reduction in \"noisy-neighbor\" influence in mixed workload clusters), environmental (e.g. by staged or sandboxed workload constructs such as \"dev\", \"test\", and \"prod\" environments), security (e.g. separating untrusted code or sensitive data), organisational (e.g. teams fall under different business units or management domains), and cost based (e.g. teams are subject to separate budgetary constraints);</li> <li>Reliability based concerns include blast radius and infrastructure diversity (e.g. preventing an application based or underlying infrastructure issue in one cluster or provider zone from impacting the entire solution), and scale based (e.g. the workload may outgrow a single cluster)</li> </ul> <p></p> <p>Multi-cluster application architectures tend to be designed to either be replicated in nature - with this pattern each participating cluster runs a full copy of each given application; or alternatively they implement more of a group-by-service pattern where the services of a single application or system are split or divided amongst multiple clusters.</p> <p>When it comes to the configuration of Kubernetes (and the surrounding infrastructure) to support a given multi-cluster application architecture - the space has evolved over time to include a number of approaches. Implementations tend draw upon a combination of components at various levels of the stack, and generally speaking they also vary in terms of the \"weight\" or complexity of the implementation, number and scope of features offered, as well as the associated management overhead. In simple terms these approaches can be loosely grouped into two main categories:</p> <ul> <li>Network-centric approaches focus on network interconnection tooling to implement connectivity between clusters in order to facilitate cross-cluster application communication. The various network-centric approaches include those that are tightly coupled with the CNI (e.g. Cillium Mesh), as well as more CNI agnostic implementations such as Submariner and Skupper. Service mesh implementations also fall into the network-centric category, and these include Istio\u2019s multi-cluster support, Linkerd service mirroring, Kuma from Kong, AWS App Mesh, and Consul\u2019s mesh gateway. There are also various multi-cluster ingress approaches, as well as virtual-kubelet based approaches including Admiralty, Tensile-kube, and Liqo.</li> <li>Kubernetes-centric approaches focus on supporting and extending the core Kubernetes primitives in order to support multi-cluster use cases. These approaches fall under the stewardship of the Kubernetes Multicluster Special Interest Group whose charter is focused on designing, implementing, and maintaining API\u2019s, tools, and documentation related to multi-cluster administration and application management. Subprojects include:<ul> <li>kubefed (Kubernetes Cluster Federation) which implements a mechanism to coordinate the configuration of multiple Kubernetes clusters from a single set of APIs in a hosting cluster. kubefed is considered to be foundational for more complex multi-cluster use cases such as deploying multi-geo applications, and disaster recovery.</li> <li>work-api (Multi-Cluster Works API) aims to group a set of Kubernetes API resources to be applied to one or multiple clusters together as a concept of \u201cwork\u201d or \u201cworkload\u201d for the purpose of multi-cluster workload lifecycle mangement.</li> <li>mcs-api (Multi-cluster Services APIs) implements an API specification to extend the single-cluster bounded Kubernetes service concept to function across multiple clusters.</li> </ul> </li> </ul>"},{"location":"#about-the-multi-cluster-services-api","title":"About the Multi-cluster Services API","text":"<p>Kubernetes' familiar Service object lets you discover and access services within the boundary of a single Kubernetes cluster. The mcs-api implements a Kubernetes-native extension to the Service API, extending the scope of the service resource concept beyond the cluster boundary - providing a mechanism to weave multiple clusters together using standard (and familiar) DNS based service discovery.</p> <p>KEP-1645: Multi-Cluster Services API provides the formal description of the Multi Cluster Service API. KEP-1645 doesn't define a complete implementation - it serves to define how an implementation should behave.At the time of writing the mcs-api version is: <code>multicluster.k8s.io/v1alpha1</code></p> <p>The primary deployment scenarios covered by the mcs-api include:</p> <ul> <li>Different services each deployed to separate clusters: I have 2 clusters, each running different services managed by different teams, where services from one team depend on services from the other team. I want to ensure that a service from one team can discover a service from the other team (via DNS resolving to VIP), regardless of the cluster that they reside in. In addition, I want to make sure that if the dependent service is migrated to another cluster, the dependee is not impacted.</li> <li>Single service deployed to multiple clusters: I have deployed my stateless service to multiple clusters for redundancy or scale. Now I want to propagate topologically-aware service endpoints (local, regional, global) to all clusters, so that other services in my clusters can access instances of this service in priority order based on availability and locality.</li> </ul> <p>The mcs-api is able to support these use cases through the described properties of a <code>ClusterSet</code>, which is a group of clusters with a high degree of mutual trust and shared ownership that share services amongst themselves - along with two additional API objects: the <code>ServiceExport</code> and the <code>ServiceImport</code>.</p> <p>Services are not visible to other clusters in the <code>ClusterSet</code> by default, they must be explicitly marked for export by the user. Creating a <code>ServiceExport</code> object for a given service specifies that the service should be exposed across all clusters in the <code>ClusterSet</code>. The mcs-api implementation (typically a controller) will automatically generate a corresponding <code>ServiceImport</code> object (which serves as the in-cluster representation of a multi-cluster service) in each importing cluster - for consumer workloads to be able to locate and consume the exported service.</p> <p>DNS-based service discovery for <code>ServiceImport</code> objects is facilitated by the Kubernetes DNS-Based Multicluster Service Discovery Specification which extends the standard Kubernetes DNS paradigms by implementing records named by service and namespace for <code>ServiceImport</code> objects, but as differentiated from regular in-cluster DNS service names by using the special zone <code>.clusterset.local</code>. I.e. When a <code>ServiceExport</code> is created, this will cause a FQDN for the multi-cluster service to become available from within the <code>ClusterSet</code>. The domain name will be of the format <code>&lt;service&gt;.&lt;ns&gt;.svc.clusterset.local</code>.</p>"},{"location":"#aws-cloud-map-mcs-controller-for-kubernetes","title":"AWS Cloud Map MCS Controller for Kubernetes","text":"<p>The AWS Cloud Map MCS Controller for Kubernetes (MCS-Controller) is an open source project that implements the multi-cluster services API specification.</p> <p>The MCS-Controller is a controller that syncs services across clusters and makes them available for multi-cluster service discovery and connectivity. The implementation model is decentralized, and utilises AWS Cloud Map as registry for management and distribution of multi-cluster service data.</p> <p>At the time of writing, the MCS-Controller release version is v0.3.0 which introduces new features including the ClusterProperty CRD, and support for headless services. Milestones are currently in place to bring the project up to v1.0 (GA), which will include full compliance with the mcs-api specification, support for multiple AWS accounts, and Cloud Map client-side traffic shaping.</p>"},{"location":"#aws-cloud-map","title":"AWS Cloud Map","text":"<p>AWS Cloud Map is a cloud resource discovery service that allows applications to discover web-based services via the AWS SDK, API calls, or DNS queries. Cloud Map is a fully managed service which eliminates the need to set up, update, and manage your own service discovery tools and software..</p>"},{"location":"#tutorial","title":"Tutorial","text":""},{"location":"#overview","title":"Overview","text":"<p>Let's consider a deployment scenario where we provision a Service into a single EKS cluster, then make the service available from within a second EKS cluster using the AWS Cloud Map MCS Controller.</p> <p>This tutorial will take you through the end-end implementation of the solution as outlined herein, including a functional implementation of the AWS Cloud Map MCS Controller across x2 EKS clusters situated in separate VPCs.</p>"},{"location":"#solution-baseline","title":"Solution Baseline","text":"<p>In reference to the Solution Baseline diagram:</p> <ul> <li>We have x2 EKS clusters (Cluster 1 &amp; Cluster 2), each deployed into separate VPCs within a single AWS region.<ul> <li>Cluster 1 VPC CIDR: 10.10.0.0/16, Kubernetes service IPv4 CIDR: 172.20.0.0/16</li> <li>Cluster 2 VPC CIDR: 10.12.0.0/16, Kubernetes service IPv4 CIDR: 172.20.0.0/16</li> </ul> </li> <li>VPC peering is configured to permit network connectivity between workloads within each cluster.</li> <li>The CoreDNS multicluster plugin is deployed to each cluster.</li> <li>The AWS Cloud Map MCS Controller for Kubernetes is deployed to each cluster.</li> <li>Clusters 1 &amp; 2 are each configured as members of the same mcs-api <code>ClusterSet</code>.<ul> <li>Cluster 1 mcs-api <code>ClusterSet</code>: clusterset1, <code>Cluster</code> Id: cls1.</li> <li>Cluster 2 mcs-api <code>ClusterSet</code>: clusterset1, <code>Cluster</code> Id: cls2.</li> </ul> </li> <li>Clusters 1 &amp; 2 are both provisioned with the namespace <code>demo</code>.</li> <li>Cluster 1 has a <code>ClusterIP</code> Service <code>nginx-hello</code> deployed to the <code>demo</code> namespace which frontends a x3 replica Nginx deployment <code>nginx-demo</code>.<ul> <li>Service | nginx-hello: 172.20.150.33:80</li> <li>Endpoints | nginx-hello: 10.10.66.181:80,10.10.78.125:80,10.10.86.76:80</li> </ul> </li> </ul>"},{"location":"#service-provisioning","title":"Service Provisioning","text":"<p>With the required dependencies in place, the admin user is able to create a <code>ServiceExport</code> object in Cluster 1 for the <code>nginx-hello</code> Service, such that the MCS-Controller implementation will automatically provision a corresponding <code>ServiceImport</code> in Cluster 2 for consumer workloads to be able to locate and consume the exported service.</p> <p></p> <p>In reference to the Service Provisioning diagram:</p> <ol> <li>The administrator submits the request to the Cluster 1 Kube API server for a <code>ServiceExport</code> object to be created for ClusterIP Service <code>nginx-hello</code> in the <code>demo</code> Namespace.</li> <li>The MCS-Controller in Cluster 1, watching for <code>ServiceExport</code> object creation provisions a corresponding <code>nginx-hello</code> service in the Cloud Map <code>demo</code> namespace. The Cloud Map service is provisioned with sufficient detail for the Service object and corresponding Endpoint Slice to be provisioned within additional clusters in the <code>ClusterSet</code>.</li> <li>The MCS-Controller in Cluster 2 responds to the creation of the <code>nginx-hello</code> Cloud Map Service by provisioning the <code>ServiceImport</code> object and corresponding <code>EndpointSlice</code> objects via the Kube API Server.</li> <li>The CoreDNS multicluster plugin, watching for <code>ServiceImport</code> and <code>EndpointSlice</code> creation provisions corresponding DNS records within the <code>.clusterset.local</code> zone.</li> </ol>"},{"location":"#service-consumption","title":"Service Consumption","text":"<p>In reference to the Service Consumption diagram:</p> <ol> <li>The <code>client-hello</code> pod in Cluster 2 needs to consume the <code>nginx-hello</code> service, for which all Endpoints are deployed in Cluster 1. The <code>client-hello</code> pod requests the resource http://nginx-hello.demo.svc.clusterset.local:80. DNS based service discovery [1b] responds with the IP address of the local <code>nginx-hello</code> <code>ServiceExport</code> Service <code>ClusterSetIP</code>.</li> <li>Requests to the local <code>ClusterSetIP</code> at <code>nginx-hello.demo.svc.clusterset.local</code> are proxied to the Endpoints located on Cluster 1.</li> </ol> <p>Note: In accordance with the mcs-api specification, a multi-cluster service will be imported by all clusters in which the service's namespace exists, meaning that each exporting cluster will also import the corresponding multi-cluster service. As such, the <code>nginx-hello</code> service will also be accessible via <code>ServiceExport</code> Service <code>ClusterSetIP</code> on Cluster 1. Identical to Cluster 2, the <code>ServiceExport</code> Service is resolvable by name at <code>nginx-hello.demo.svc.clusterset.local</code>.</p>"},{"location":"#implementation","title":"Implementation","text":""},{"location":"#solution-baseline_1","title":"Solution Baseline","text":"<p>To prepare your environment to match the Solution Baseline deployment scenario, the following prerequisites should be addressed.</p>"},{"location":"#clone-the-aws-cloud-map-mcs-controller-for-k8s-git-repository","title":"Clone the <code>aws-cloud-map-mcs-controller-for-k8s</code> git repository","text":"<p>Sample configuration files will be used through the course of the tutorial, which have been made available in the <code>aws-cloud-map-mcs-controller-for-k8s</code> repository.</p> <p>Clone the repository to the host from which you will be bootstrapping the clusters:</p> <pre><code>git clone https://github.com/aws/aws-cloud-map-mcs-controller-for-k8s.git\n</code></pre> <p>Note: All commands as provided should be run from the root directory of the cloned git repository.</p> <p>Note: Certain values located within the provided configuration files have been configured for substitution with OS environment variables. Work instructions below will identify which environment variables should be set before issuing any commands which will depend on variable substitution.</p>"},{"location":"#create-eks-clusters","title":"Create EKS Clusters","text":"<p>x2 EKS clusters should be provisioned, each deployed into separate VPCs within a single AWS region.</p> <ul> <li>VPCs and clusters should be provisioned with non-overlapping CIDRs.</li> <li>For compatibility with the remainder of the tutorial, it is recommended that <code>eksctl</code> be used to provision the clusters and associated security configuration. By default, the <code>eksctl create cluster</code> command will create a dedicated VPC.</li> </ul> <p>Sample <code>eksctl</code> config file <code>samples/eksctl-cluster.yaml</code> has been provided:</p> <ul> <li>Environment variables AWS_REGION, CLUSTER_NAME, NODEGROUP_NAME, and VPC_CIDR should be configured. Example values have been provided in the below command reference - substitute values to suit your preference.</li> <li>Example VPC CIDRs match the values provided in the Baseline Configuration description.</li> </ul> <p>Run the following commands to create clusters using <code>eksctl</code>.</p> <p>Cluster 1:</p> <pre><code>export AWS_REGION=ap-southeast-2\nexport CLUSTER_NAME=cls1\nexport NODEGROUP_NAME=cls1-nodegroup1\nexport VPC_CIDR=10.10.0.0/16\nenvsubst &lt; samples/eksctl-cluster.yaml | eksctl create cluster -f -\n</code></pre> <p>Cluster 2:</p> <pre><code>export AWS_REGION=ap-southeast-2\nexport CLUSTER_NAME=cls2\nexport NODEGROUP_NAME=cls2-nodegroup1\nexport VPC_CIDR=10.12.0.0/16\nenvsubst &lt; samples/eksctl-cluster.yaml | eksctl create cluster -f -\n</code></pre>"},{"location":"#create-vpc-peering-connection","title":"Create VPC Peering Connection","text":"<p>VPC peering is required to permit network connectivity between workloads provisioned within each cluster.</p> <ul> <li>To create the VPC Peering connection, follow the instruction Create a VPC peering connection with another VPC in your account for guidance.</li> <li> <p>VPC route tables in each VPC require updating, follow the instruction Update your route tables for a VPC peering connection for guidance. For simplicity, it's recommended to configure route destinations as the IPv4 CIDR block of the peer VPC.</p> </li> <li> <p>Security Groups require updating to permit cross-cluster network communication. EKS cluster security groups in each cluster should be updated to permit inbound traffic originating from external clusters. For simplicity, it's recommended the Cluster 1 &amp; Cluster 2 EKS Cluster Security groups be updated to allow inbound traffic from the IPv4 CIDR block of the peer VPC.</p> </li> </ul> <p>The VPC Reachability Analyzer can be used to test and diagnose end-end connectivity between worker nodes within each cluster.</p>"},{"location":"#enable-eks-oidc-provider","title":"Enable EKS OIDC Provider","text":"<p>In order to map required Cloud Map AWS IAM permissions to the MCS-Controller Kubernetes service account, we need to enable the OpenID Connect (OIDC) identity provider in our EKS clusters using <code>eksctl</code>.</p> <ul> <li>Environment variables REGION and CLUSTERNAME should be configured.</li> </ul> <p>Run the following commands to enable OIDC providers using <code>eksctl</code>.</p> <p>Cluster 1:</p> <pre><code>export AWS_REGION=ap-southeast-2\nexport CLUSTER_NAME=cls1\neksctl utils associate-iam-oidc-provider \\\n    --region $AWS_REGION \\\n    --cluster $CLUSTER_NAME \\\n    --approve\n</code></pre> <p>Cluster 2:</p> <pre><code>export AWS_REGION=ap-southeast-2\nexport CLUSTER_NAME=cls2\neksctl utils associate-iam-oidc-provider \\\n    --region $AWS_REGION \\\n    --cluster $CLUSTER_NAME \\\n    --approve\n</code></pre>"},{"location":"#implement-coredns-multicluster-plugin","title":"Implement CoreDNS multicluster plugin","text":"<p>The CoreDNS multicluster plugin implements the Kubernetes DNS-Based Multicluster Service Discovery Specification which enables CoreDNS to lifecycle manage DNS records for <code>ServiceImport</code> objects. To enable the CoreDNS multicluster plugin within both EKS clusters, perform the following procedure.</p>"},{"location":"#update-coredns-rbac","title":"Update CoreDNS RBAC","text":"<p>Run the following command against both clusters to update the <code>system:coredns</code> clusterrole to include access to additional multi-cluster API resources:</p> <pre><code>kubectl apply -f samples/coredns-clusterrole.yaml\n</code></pre>"},{"location":"#update-the-coredns-configmap","title":"Update the CoreDNS configmap","text":"<p>Run the following command against both clusters to update the default CoreDNS configmap to include the multicluster plugin directive, and <code>clusterset.local</code> zone:</p> <pre><code>kubectl apply -f samples/coredns-configmap.yaml\n</code></pre>"},{"location":"#update-the-coredns-deployment","title":"Update the CoreDNS deployment","text":"<p>Run the following command against both clusters to update the default CoreDNS deployment to use the container image <code>ghcr.io/aws/aws-cloud-map-mcs-controller-for-k8s/coredns-multicluster/coredns:v1.8.6</code> - which includes the multicluster plugin:</p> <pre><code>kubectl apply -f samples/coredns-deployment.yaml\n</code></pre>"},{"location":"#install-the-aws-cloud-map-mcs-controller-for-k8s","title":"Install the aws-cloud-map-mcs-controller-for-k8s","text":""},{"location":"#configure-mcs-controller-rbac","title":"Configure MCS-Controller RBAC","text":"<p>Before the Cloud Map MCS-Controller is installed, we will first pre-provision the controller Service Account, granting IAM access rights <code>AWSCloudMapFullAccess</code> to ensure that the MCS Controller can lifecycle manage Cloud Map resources.</p> <ul> <li>Environment variable CLUSTER_NAME should be configured.</li> </ul> <p>Run the following commands to create the MCS-Controller namespace and service accounts in each cluster.</p> <p>Note: Be sure to change the <code>kubectl</code> context to the correct cluster before issuing commands.</p> <p>Cluster 1:</p> <pre><code>export CLUSTER_NAME=cls1\nkubectl create namespace cloud-map-mcs-system\neksctl create iamserviceaccount \\\n--cluster $CLUSTER_NAME \\\n--namespace cloud-map-mcs-system \\\n--name cloud-map-mcs-controller-manager \\\n--attach-policy-arn arn:aws:iam::aws:policy/AWSCloudMapFullAccess \\\n--override-existing-serviceaccounts \\\n--approve\n</code></pre> <p>Cluster 2:</p> <pre><code>export CLUSTER_NAME=cls2\nkubectl create namespace cloud-map-mcs-system\neksctl create iamserviceaccount \\\n--cluster $CLUSTER_NAME \\\n--namespace cloud-map-mcs-system \\\n--name cloud-map-mcs-controller-manager \\\n--attach-policy-arn arn:aws:iam::aws:policy/AWSCloudMapFullAccess \\\n--override-existing-serviceaccounts \\\n--approve\n</code></pre>"},{"location":"#install-the-mcs-controller","title":"Install the MCS-Controller","text":"<p>Now to install the MCS-Controller.</p> <ul> <li>Environment variable AWS_REGION should be configured.</li> </ul> <p>Run the following command against both clusters to install the MCS-Controller latest release:</p> <pre><code>export AWS_REGION=ap-southeast-2\nkubectl apply -k \"github.com/aws/aws-cloud-map-mcs-controller-for-k8s/config/controller_install_release\"\n</code></pre>"},{"location":"#assign-mcs-api-clusterset-membership-and-cluster-identifier","title":"Assign mcs-api <code>ClusterSet</code> membership and <code>Cluster</code> identifier","text":"<p>To ensure that  <code>ServiceExport</code> and  <code>ServiceImport</code> objects propagate correctly between clusters, each cluster should be configured as a member of a single mcs-api <code>ClusterSet</code> (clusterset1 in our example deployment scenario), and should be assigned a unique mcs-api <code>Cluster</code> Id within the <code>ClusterSet</code> (cls1 &amp; cls2 in our example deployment scenario).</p> <ul> <li>Environment variable CLUSTER_ID should be configured.</li> <li>Environment variable CLUSTERSET_ID should be configured.</li> </ul> <p>Run the following commands to configure Cluster  Id and ClusterSet membership.</p> <p>Cluster 1:</p> <pre><code>export CLUSTER_ID=cls1\nexport CLUSTERSET_ID=clusterset1\nenvsubst &lt; samples/mcsapi-clusterproperty.yaml | kubectl apply -f -\n</code></pre> <p>Cluster 2:</p> <pre><code>export CLUSTER_ID=cls2\nexport CLUSTERSET_ID=clusterset1\nenvsubst &lt; samples/mcsapi-clusterproperty.yaml | kubectl apply -f -\n</code></pre>"},{"location":"#create-nginx-hello-service","title":"Create <code>nginx-hello</code> Service","text":"<p>Now that the clusters, CoreDNS and the MCS-Controller have been configured, we can create the <code>demo</code> namespace in both clusters and implement the <code>nginx-hello</code> Service and associated Deployment into Cluster 1.</p> <p>Run the following commands to prepare the demo environment on both clusters.</p> <p>Note: be sure to change the <code>kubectl</code> context to the correct cluster before issuing commands.</p> <p>Cluster 1:</p> <pre><code>kubectl create namespace demo\nkubectl apply -f samples/nginx-deployment.yaml\nkubectl apply -f samples/nginx-service.yaml\n</code></pre> <p>Cluster 2:</p> <pre><code>kubectl create namespace demo\n</code></pre>"},{"location":"#service-provisioning_1","title":"Service Provisioning","text":"<p>With the Solution Baseline in place, let's continue by implementing the Service Provisioning scenario. We'll create a <code>ServiceExport</code> object in Cluster 1 for the <code>nginx-hello</code> Service. This will trigger the Cluster 1 MCS-Controller to complete service provisioning and propagation into Cloud Map, and subsequent import and provisioning by the MCS-Controller in Cluster 2.</p>"},{"location":"#create-nginx-hello-serviceexport","title":"Create <code>nginx-hello</code> ServiceExport","text":"<p>Run the following command against Cluster 1 to to create the <code>ServiceExport</code> object for the <code>nginx-hello</code> Service:</p> <pre><code>kubectl apply -f \\config\\nginx-serviceexport.yaml\n</code></pre>"},{"location":"#verify-nginx-hello-serviceexport","title":"Verify <code>nginx-hello</code> ServiceExport","text":"<p>Let's verify the <code>ServiceExport</code> creation has succeeded, and that corresponding objects have been created in Cluster 1, Cloud Map, and Cluster 2.</p>"},{"location":"#cluster-1","title":"Cluster 1","text":"<p>Inspecting the MCS-Controller logs in Cluster 1, we see that the controller has detected the <code>ServiceExport</code> object, and created the corresponding <code>demo</code> Namespace and <code>nginx-hello</code> Service in Cloud Map:</p> <pre><code>$ kubectl logs cloud-map-mcs-controller-manager-5b9f959fc9-hmz88 -c manager --namespace cloud-map-mcs-system\n{\"level\":\"info\",\"ts\":1665108812.7046816,\"logger\":\"cloudmap\",\"msg\":\"namespace created\",\"nsId\":\"ns-nlnawwa2wa3ajoh3\"}\n{\"level\":\"info\",\"ts\":1665108812.7626762,\"logger\":\"cloudmap\",\"msg\":\"service created\",\"namespace\":\"demo\",\"name\":\"nginx-hello\",\"id\":\"srv-xqirlhajwua5vkvo\"}\n{\"level\":\"info\",\"ts\":1665108812.7627065,\"logger\":\"cloudmap\",\"msg\":\"fetching a service\",\"namespace\":\"demo\",\"name\":\"nginx-hello\"}\n{\"level\":\"info\",\"ts\":1665108812.8299918,\"logger\":\"cloudmap\",\"msg\":\"registering endpoints\",\"namespaceName\":\"demo\",\"serviceName\":\"nginx-hello\",\"endpoints\":[{\"Id\":\"tcp-10_10_86_76-80\",\"IP\":\"10.10.86.76\",\"EndpointPort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"\",\"Protocol\":\"TCP\"},\"ServicePort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"80\",\"Protocol\":\"TCP\"},\"ClusterId\":\"cls1\",\"ClusterSetId\":\"clusterset1\",\"ServiceType\":\"ClusterSetIP\",\"ServiceExportCreationTimestamp\":1665108776000,\"Ready\":true,\"Hostname\":\"\",\"Nodename\":\"ip-10-10-77-143.ap-southeast-2.compute.internal\",\"Attributes\":{\"K8S_CONTROLLER\":\"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\"}},{\"Id\":\"tcp-10_10_66_181-80\",\"IP\":\"10.10.66.181\",\"EndpointPort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"\",\"Protocol\":\"TCP\"},\"ServicePort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"80\",\"Protocol\":\"TCP\"},\"ClusterId\":\"cls1\",\"ClusterSetId\":\"clusterset1\",\"ServiceType\":\"ClusterSetIP\",\"ServiceExportCreationTimestamp\":1665108776000,\"Ready\":true,\"Hostname\":\"\",\"Nodename\":\"ip-10-10-77-143.ap-southeast-2.compute.internal\",\"Attributes\":{\"K8S_CONTROLLER\":\"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\"}},{\"Id\":\"tcp-10_10_78_125-80\",\"IP\":\"10.10.78.125\",\"EndpointPort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"\",\"Protocol\":\"TCP\"},\"ServicePort\":{\"Name\":\"\",\"Port\":80,\"TargetPort\":\"80\",\"Protocol\":\"TCP\"},\"ClusterId\":\"cls1\",\"ClusterSetId\":\"clusterset1\",\"ServiceType\":\"ClusterSetIP\",\"ServiceExportCreationTimestamp\":1665108776000,\"Ready\":true,\"Hostname\":\"\",\"Nodename\":\"ip-10-10-77-143.ap-southeast-2.compute.internal\",\"Attributes\":{\"K8S_CONTROLLER\":\"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\"}}]}\n</code></pre> <p>Using the AWS CLI we can verify Namespace and Service resources provisioned to Cloud Map by the Cluster 1 MCS-Controller:</p> <pre><code>$ aws servicediscovery list-namespaces\n{\n    \"Namespaces\": [\n        {\n            \"Id\": \"ns-nlnawwa2wa3ajoh3\",\n            \"Arn\": \"arn:aws:servicediscovery:ap-southeast-2:911483634971:namespace/ns-nlnawwa2wa3ajoh3\",\n            \"Name\": \"demo\",\n            \"Type\": \"HTTP\",\n            \"Properties\": {\n                \"DnsProperties\": {\n                    \"SOA\": {}\n                },\n                \"HttpProperties\": {\n                    \"HttpName\": \"demo\"\n                }\n            },\n            \"CreateDate\": \"2022-10-07T02:13:32.310000+00:00\"\n        }\n    ]\n}\n$ aws servicediscovery list-services\n{\n    \"Services\": [\n        {\n            \"Id\": \"srv-xqirlhajwua5vkvo\",\n            \"Arn\": \"arn:aws:servicediscovery:ap-southeast-2:911483634971:service/srv-xqirlhajwua5vkvo\",\n            \"Name\": \"nginx-hello\",\n            \"Type\": \"HTTP\",\n            \"DnsConfig\": {},\n            \"CreateDate\": \"2022-10-07T02:13:32.744000+00:00\"\n        }\n    ]\n}\n$ aws servicediscovery discover-instances --namespace-name demo --service-name nginx-hello\n{\n    \"Instances\": [\n        {\n            \"InstanceId\": \"tcp-10_10_78_125-80\",\n            \"NamespaceName\": \"demo\",\n            \"ServiceName\": \"nginx-hello\",\n            \"HealthStatus\": \"UNKNOWN\",\n            \"Attributes\": {\n                \"AWS_INSTANCE_IPV4\": \"10.10.78.125\",\n                \"AWS_INSTANCE_PORT\": \"80\",\n                \"CLUSTERSET_ID\": \"clusterset1\",\n                \"CLUSTER_ID\": \"cls1\",\n                \"ENDPOINT_PORT_NAME\": \"\",\n                \"ENDPOINT_PROTOCOL\": \"TCP\",\n                \"HOSTNAME\": \"\",\n                \"K8S_CONTROLLER\": \"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\",\n                \"NODENAME\": \"ip-10-10-77-143.ap-southeast-2.compute.internal\",\n                \"READY\": \"true\",\n                \"SERVICE_EXPORT_CREATION_TIMESTAMP\": \"1665108776000\",\n                \"SERVICE_PORT\": \"80\",\n                \"SERVICE_PORT_NAME\": \"\",\n                \"SERVICE_PROTOCOL\": \"TCP\",\n                \"SERVICE_TARGET_PORT\": \"80\",\n                \"SERVICE_TYPE\": \"ClusterSetIP\"\n            }\n        },\n        {\n            \"InstanceId\": \"tcp-10_10_66_181-80\",\n            \"NamespaceName\": \"demo\",\n            \"ServiceName\": \"nginx-hello\",\n            \"HealthStatus\": \"UNKNOWN\",\n            \"Attributes\": {\n                \"AWS_INSTANCE_IPV4\": \"10.10.66.181\",\n                \"AWS_INSTANCE_PORT\": \"80\",\n                \"CLUSTERSET_ID\": \"clusterset1\",\n                \"CLUSTER_ID\": \"cls1\",\n                \"ENDPOINT_PORT_NAME\": \"\",\n                \"ENDPOINT_PROTOCOL\": \"TCP\",\n                \"HOSTNAME\": \"\",\n                \"K8S_CONTROLLER\": \"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\",\n                \"NODENAME\": \"ip-10-10-77-143.ap-southeast-2.compute.internal\",\n                \"READY\": \"true\",\n                \"SERVICE_EXPORT_CREATION_TIMESTAMP\": \"1665108776000\",\n                \"SERVICE_PORT\": \"80\",\n                \"SERVICE_PORT_NAME\": \"\",\n                \"SERVICE_PROTOCOL\": \"TCP\",\n                \"SERVICE_TARGET_PORT\": \"80\",\n                \"SERVICE_TYPE\": \"ClusterSetIP\"\n            }\n        },\n        {\n            \"InstanceId\": \"tcp-10_10_86_76-80\",\n            \"NamespaceName\": \"demo\",\n            \"ServiceName\": \"nginx-hello\",\n            \"HealthStatus\": \"UNKNOWN\",\n            \"Attributes\": {\n                \"AWS_INSTANCE_IPV4\": \"10.10.86.76\",\n                \"AWS_INSTANCE_PORT\": \"80\",\n                \"CLUSTERSET_ID\": \"clusterset1\",\n                \"CLUSTER_ID\": \"cls1\",\n                \"ENDPOINT_PORT_NAME\": \"\",\n                \"ENDPOINT_PROTOCOL\": \"TCP\",\n                \"HOSTNAME\": \"\",\n                \"K8S_CONTROLLER\": \"aws-cloud-map-mcs-controller-for-k8s d07e680 (d07e680)\",\n                \"NODENAME\": \"ip-10-10-77-143.ap-southeast-2.compute.internal\",\n                \"READY\": \"true\",\n                \"SERVICE_EXPORT_CREATION_TIMESTAMP\": \"1665108776000\",\n                \"SERVICE_PORT\": \"80\",\n                \"SERVICE_PORT_NAME\": \"\",\n                \"SERVICE_PROTOCOL\": \"TCP\",\n                \"SERVICE_TARGET_PORT\": \"80\",\n                \"SERVICE_TYPE\": \"ClusterSetIP\"\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"#cluster-2","title":"Cluster 2","text":"<p>Inspecting the MCS-Controller logs in Cluster 2, we see that the controller has detected the <code>nginx-hello</code> Cloud Map Service, and created the corresponding Kubernetes <code>ServiceImport</code>:</p> <pre><code>$ kubectl logs cloud-map-mcs-controller-manager-5b9f959fc9-v72s4 -c manager --namespace cloud-map-mcs-system\n{\"level\":\"info\",\"ts\":1665108822.2781157,\"logger\":\"controllers.Cloudmap\",\"msg\":\"created ServiceImport\",\"namespace\":\"demo\",\"name\":\"nginx-hello\"}\n{\"level\":\"info\",\"ts\":1665108824.2420218,\"logger\":\"controllers.Cloudmap\",\"msg\":\"created derived Service\",\"namespace\":\"demo\",\"name\":\"imported-9cfu7k5mkr\"}\n{\"level\":\"info\",\"ts\":1665108824.2501283,\"logger\":\"controllers.Cloudmap\",\"msg\":\"ServiceImport IPs need update\",\"ServiceImport IPs\":[],\"cluster IPs\":[\"172.20.80.119\"]}\n{\"level\":\"info\",\"ts\":1665108824.2618752,\"logger\":\"controllers.Cloudmap\",\"msg\":\"updated ServiceImport\",\"namespace\":\"demo\",\"name\":\"nginx-hello\",\"IP\":[\"172.20.80.119\"],\"ports\":[{\"protocol\":\"TCP\",\"port\":80}]}\n</code></pre> <p>Inspecting the Cluster 2 Kubernetes <code>ServiceImport</code> object:</p> <pre><code>$ kubectl get serviceimports.multicluster.x-k8s.io nginx-hello -n demo -o yaml\napiVersion: multicluster.x-k8s.io/v1alpha1\nkind: ServiceImport\nmetadata:\n  annotations:\n    multicluster.k8s.aws/derived-service: '[{\"cluster\":\"cls1\",\"derived-service\":\"imported-9cfu7k5mkr\"}]'\n  creationTimestamp: \"2022-10-07T02:13:42Z\"\n  generation: 2\n  name: nginx-hello\n  namespace: demo\n  resourceVersion: \"12787\"\n  uid: a53901af-57a8-49c7-aeb1-f67c4a44c2d2\nspec:\n  ips:\n  - 172.20.80.119\n  ports:\n  - port: 80\n    protocol: TCP\n  type: ClusterSetIP\nstatus:\n  clusters:\n  - cluster: cls1\n</code></pre> <p>And the corresponding Cluster 2 Kubernetes Endpoint Slice:</p> <pre><code>$ kubectl get endpointslices.discovery.k8s.io -n demo\nNAME                        ADDRESSTYPE   PORTS   ENDPOINTS                               AGE\nimported-9cfu7k5mkr-dc7q9   IPv4          80      10.10.78.125,10.10.86.76,10.10.66.181   14m\n</code></pre> <p>Important points to note:</p> <ul> <li>the <code>ServiceImport</code> Service is assigned an IP address from the local Kubernetes service IPv4 CIDR: 172.22.0.0/16 (172.20.80.119) so as to permit service discovery and access to the remote service endpoints from within the local cluster.</li> <li>the <code>EndpointSlice</code> IP addresses match those of the <code>nginx-demo</code> Endpoints in Cluster 1 (i.e. from the Cluster 1 VPC CIDR: 10.10.0.0/16).</li> </ul>"},{"location":"#service-consumption_1","title":"Service Consumption","text":"<p>With the Solution Baseline and Service Provisioning in place, workloads in Cluster 2 are now able to consume the nginx-hello Service Endpoints located in Cluster 1 via the locally provisioned ServiceImport object. To complete the Service Consumption scenario we'll deploy the client-hello Pod into Cluster 2, and observe how it's able to perform cross-cluster service discovery, and access each of the nginx-hello Service Endpoints in Cluster 1.</p>"},{"location":"#create-client-hello-pod","title":"Create <code>client-hello</code> Pod","text":"<p>Run the following command against Cluster 2 create the <code>client-hello</code> Pod:</p> <pre><code>kubectl apply -f samples/client-hello.yaml\n</code></pre>"},{"location":"#verify-multi-cluster-service-consumption","title":"Verify multi-cluster service consumption","text":"<p>Let's exec into the <code>client-hello</code> Pod and perform an <code>nslookup</code> to cluster-local CoreDNS for the <code>ServiceImport</code> Service <code>nginx-hello.demo.svc.clusterset.local</code>:</p> <pre><code>$ kubectl exec -it client-hello -n demo /bin/sh\n/ # nslookup nginx-hello.demo.svc.clusterset.local\nServer:         172.20.0.10\nAddress:        172.20.0.10:53\n\nName:   nginx-hello.demo.svc.clusterset.local\nAddress: 172.20.80.119\n</code></pre> <p>Note that the Pod resolves the address of the <code>ServiceImport</code> object on Cluster 2.</p> <p>Finally, we generate HTTP requests from the <code>client-hello</code> Pod to the local <code>nginx-hello</code> <code>ServiceImport</code> Service:</p> <pre><code>/ # apk --no-cache add curl\n/ # curl nginx-hello.demo.svc.clusterset.local\nServer address: 10.10.86.76:80\nServer name: nginx-demo-59c6cb8d7b-m4ktw\nDate: 07/Oct/2022:02:31:45 +0000\nURI: /\nRequest ID: 17d43e6e8801a98d05059dfaf88d0abe\n/ # \n/ # curl nginx-hello.demo.svc.clusterset.local\nServer address: 10.10.78.125:80\nServer name: nginx-demo-59c6cb8d7b-8w6rp\nDate: 07/Oct/2022:02:32:26 +0000\nURI: /\nRequest ID: 0ddc09ffe7fd45c52903ce34c955f555\n/ # \n/ # curl nginx-hello.demo.svc.clusterset.local\nServer address: 10.10.66.181:80\nServer name: nginx-demo-59c6cb8d7b-mtm8l\nDate: 07/Oct/2022:02:32:53 +0000\nURI: /\nRequest ID: 2fde1c34008a5ec18b8ae23797489c3a\n</code></pre> <p>Note that the responding Server Names and Server addresses are those of the <code>nginx-demo</code> Pods on Cluster 1 - confirming that the requests to the local <code>ClusterSetIP</code> at <code>nginx-hello.demo.svc.clusterset.local</code> originating on Cluster 2 are proxied cross-cluster to the Endpoints located on Cluster 1!</p>"},{"location":"#conclusion","title":"Conclusion","text":"<p>The proliferation of container adoption is presenting new challenges in supporting workloads that have broken through the perimeter of the single cluster construct.</p> <p>For teams that are looking to implement a Kubenetes-centric approach to managing multi-cluster workloads, the mcs-api describes an effective approach to extending the scope of the service resource concept beyond the cluster boundary - providing a mechanism to weave multiple clusters together using standard (and familiar) DNS based service discovery.</p> <p>The AWS Cloud Map MCS Controller for Kubernetes is an open source project that integrates with AWS Cloud Map to offer a decentralised implementation of the multi-cluster services API specification that's particularly suited for teams looking for a lightweight and effective Kubenetes-centric mechanism to deploy multi-cluster workloads to the AWS cloud.</p>"}]}